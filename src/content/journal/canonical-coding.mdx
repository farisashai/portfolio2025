---
title: "Canonical Coding"
description: "The value in overly prescriptive and singular correct stylistic choices"
year: "November 2025"
keywords: "React, Next.js, Code Quality, Conventions, Developer Experience, AI, Architecture"
labels: ["Engineering"]
featured: false
draft: true
---

## How Opinionated Conventions Eliminate Cognitive Overload and Unlock Velocity

React's power comes from its lack of prescription. But a flexible ecosystem without constraints invites fractal decision-making: too many legitimate ways to structure state, variations in folder hierarchies and naming patterns, inconsistent component boundaries, inlined types in one file and shared interfaces in another, redundant wrapper components that multiply DOM layers, and multiple ways to implement data fetching—especially in Next.js with Server Components, Client Components, and layouts.

This results in **cognitive overload**: the mental tax every new contributor pays to understand the codebase before making safe changes.

Most "bugs" in React codebases are not about logical mistakes—they stem from **inconsistent patterns** and **structural ambiguity**.

## The Thesis: Canonical Conventions Are a Force Multiplier

When a team converges on the *right* set of conventions, several things happen simultaneously:

1. **Developers stop deciding and start executing.**
   Every missing decision is one less branch in your mental tree.

2. **Abstractions normalize how developers think.**
   Everyone makes components the same way, hoists state the same way, organizes files the same way.

3. **AI understands the codebase more reliably.**
   Models navigate predictable structures far more effectively than deeply-customized systems.

4. **Correctness emerges from structure, not from lint fixes.**
   The "rigid" rules that seem stylistic often eliminate entire categories of bugs.

This is the "canonical React" mindset: a codebase where *style rules indirectly enforce correctness*.

## Convention as a Code Quality Guarantee

Style rules—if chosen well—are not cosmetic. They become invariants that **mathematically reduce the space of possible bugs**.

### Rule 1: Hoist derived and non-shared state as low in the tree as possible

**Rationale:**

- Eliminates unnecessary re-renders
- Removes ambiguity over data-ownership
- Prevents prop-drilling and accidental coupling
- Guarantees React's rendering model is used efficiently

This is not stylistic; it is a **performance guarantee** and a **mental model guarantee**.

### Rule 2: No redundant nesting

For example:

```jsx
<div>
  <div>
    <SingleChild />
  </div>
</div>
```

This rule:

- Reduces layout complexity
- Removes meaningless DOM layers that harm performance and accessibility
- Cleans up diffs, reducing noise for humans and AI
- Enforces a "minimal surface area" mentality

### Rule 3: No inline type annotations unless they are shared, named interfaces

Inline types:

- Create duplication
- Hide semantic meaning
- Increase churn when shape changes
- Make AI-assisted refactors significantly harder

Named interfaces:

- Become canonical single sources of truth
- Force teams to place types in proper domains (e.g., `types/`, `model/`, `components/foo/types.ts`)
- Sharpen boundaries between UI, data, and logic layers

These types become the *lingua franca* of a codebase.

## Abstractions as Cognitive Load Balancers

An effective React/Next.js architecture extracts complexity into the *right* primitives:

- **Canonical fetch utilities** replacing ad-hoc data loading
- **Canonical form handlers** replacing dozens of custom hooks
- **Canonical layout systems** replacing arbitrary DOM structures
- **Canonical error boundaries** rather than per-feature snowflakes
- **Canonical folder conventions** that encode meaning at a glance

When a new file "looks like every other file," understanding is immediate.

This reduces onboarding time, PR review time, and context-switch cost. It also makes the codebase highly cooperative with AI.

## The AI Angle: Code Style Is No Longer About Humans Alone

This is the forward-looking argument that many teams **haven't internalized yet**.

### Large files, inline types, and redundant DOM nodes don't just slow *humans* down. They slow **AI assistants** down.

**Why?**

1. **Context window pollution.**
   Models must ingest irrelevant wrapper elements and repeated inline type literals.

2. **Lost structure.**
   AI thrives on predictable patterns. When state is consistently placed at the lowest owner, AI can reason about component boundaries with trivial inference.

3. **Hard-to-navigate component trees.**
   If the model has to parse 1500 lines instead of jumping to a canonical subcomponent, reasoning latency increases dramatically.

4. **Ambiguous type definitions.**
   When the model sees the same shape defined inline multiple times, it cannot reliably determine the canonical shape.

5. **Reduced correctness in generated code.**
   Because AI relies heavily on pattern matching, inconsistent patterns produce inconsistent output.

### Put simply:

**The cleaner the architecture, the more potent your AI becomes.**

You're not just optimizing for humans—you're optimizing for the future of software development.

## Developer Velocity: The Real Output

Conventions improve velocity in four concrete ways:

1. **Fewer decisions** means faster iteration
2. **Lower mental overhead** means fewer errors
3. **Smaller, focused files** mean faster reviews and more confident refactors
4. **AI-optimized structure** multiplies the output of every engineer using LLM-powered toolchains

Velocity isn't about coding faster; it's about **removing friction** from every step of the loop.

## Canonical React/Next.js as a Write-Once, Read-Many System

A codebase is read far more than it is written. And now it's not just read by humans—it's read by models.

Thus:

- Predictability is a performance optimization
- Naming conventions are a safety mechanism
- Structure is a debugging tool
- Rigid rules are a scaffolding that enables velocity

## Why My Code Reviews Are Systemized—and Why That Matters

A core pillar of a canonical, convention-driven React/Next.js codebase is that **review energy should be spent on design decisions, not syntax checks**. My review approach is built on this principle. It is highly systemized for one reason: I wrote most of the architectural systems, implemented the same interfaces repeatedly, and established a set of patterns where the typechecker provides extremely strong correctness guarantees.

This leads to an important operational behavior:

**When I review code built on established interfaces, I skim large, dense configuration objects without reading them line-by-line.**

Not because I'm complacent—but because these interfaces were designed to be unbreakable. When a component schema is tight, strongly typed, and built through repeated iterations, the compiler becomes the primary reviewer. The fields are deterministic. There is no room for "creative interpretation," so it is purely mechanical data entry. If it compiles, it is nearly always correct.

Large config blocks in this world are not complexity—they are *templates being filled out*. The type system has already eliminated the possibility of structural misuse. This is exactly what conventions buy: the ability to trust the system rather than re-mentally simulate logic.

The inverse is also true.

**When I see bespoke, novel code, I switch into a different mode—reviewing for correctness, simplicity, clarity, safety, and potential bugs.**

Novel code is where deviation hides, and deviation is where errors hide. But then the next question naturally arises:

**Why did I have to review this at all?**

Whenever I review something that required a human to intervene, my immediate reflex is to ask:

- What rule could have prevented this?
- What pattern should have existed already?
- What linting constraint would have caught this 90% of the time?
- Why was this custom logic required, rather than leaning on existing abstractions?

The goal isn't to restrict the team—it's to remove future failure modes and eliminate wasted cognitive effort. Custom logic is not inherently bad, but it demands justification in a canonical system. Every repeated category of issue becomes a candidate for a lint rule or structural constraint. The end state is a codebase where **the right thing is the easy thing**, and reviewers rarely encounter surprises.

## UX-Backed Decisions: Why Rules Aren't Arbitrary

All of my architectural decisions, and thus all of my review comments, are grounded in product UX logic—not aesthetic preference. Good engineering conventions are downstream of UX clarity.

**For example:**

- A UI-level configuration constant that changes often (e.g., tuning a speed, threshold, size, animation, or weight) should be **hoisted to a high-level config** so product teams can adjust it without spelunking through code.

- But a static piece of implementation—such as a route, a semantic label, or a one-off link—**should never be a magic variable**. Indirection here destroys clarity. You create complexity for no benefit. You conceal that the value is a stable part of the component itself.

Indirection is not neutral. It is an active increase in cognitive distance. A canonical system optimizes that distance relentlessly.

This is how conventions converge toward the ideal end state: **Engineers spend time on product decisions, not hunting variables, deciphering structures, or arguing about style.**

Bus factor collapses to zero because every component follows the same rules, every interface expresses the same patterns, and every decision aligns with UX logic rather than personal taste.

## Pre-Review Self-Correction: Authors See Every Comment Before I Write It

The final, often invisible benefit:

**Once conventions are rigid and the linting rules are decisive, authors see every comment I would make *before* I ever make it.**

This is the cultural flywheel: the more systemized the codebase is, the more predictable the review comments become, and the more preemptively the author corrects their code.

By the time a PR hits my desk:

- Most low-level issues are already auto-corrected
- Most architectural missteps are prevented by conventions
- Most structural concerns are enforced by lint rules
- The PR consists almost entirely of product-level changes or genuinely novel logic

This lets reviewers focus on meaningful work instead of static analysis by hand.

## The Vacation Test: When the System Writes Code Like You

Here's my ideal heuristic for a truly canonical codebase:

**If I went on vacation for a week and someone else had to write frontend code—manually or with AI—the systems in place would guarantee it looks exactly like code I would write.**

Not "similar to." Not "in the spirit of." **Exactly like.**

This is not about ego. It's about **knowledge codification**. Every architectural decision I've made, every pattern I've established, every edge case I've encountered—all of it should be encoded into the repository itself, not trapped in my head.

The goal is to build a system where:

### 1. Prompt files and agent rules live in the repository

The `.claude/` directory (or equivalent) contains:

- Canonical component templates
- Architectural decision records with enforcement rules
- Examples of correct patterns for every common scenario
- Anti-patterns with explanations of why they're prohibited

When an AI assistant opens the codebase, it reads these rules first. When a human onboards, they have the same source of truth.

These aren't documentation—they're **executable knowledge**. They tell AI assistants exactly how to write code that passes review.

### 2. Agentic CI checks enforce the knowledge base

Traditional CI runs tests and linters. Agentic CI goes further:

- **Structural analysis**: Does this PR follow the canonical folder structure?
- **Pattern matching**: Does this component match established patterns?
- **Type consistency**: Are all types placed in their canonical locations?
- **Architectural review**: Does this change introduce new abstractions when existing ones should be used?

These checks aren't static rules—they're **AI agents that understand context**. They can explain *why* a pattern is wrong and *how* to fix it. They can point to similar correct examples in the codebase.

When a check fails, it doesn't just block the PR—it teaches the contributor.

### 3. Zero knowledge transfer overhead

The most expensive thing in software engineering isn't writing code—it's **transferring knowledge**.

Every time I explain why we structure components a certain way, that's time lost. Every code review comment that says "we don't do it this way, we do it this way"—that's knowledge I'm transmitting manually.

In a truly canonical system, I codify that knowledge **once**:

- Write the lint rule
- Add the CI check
- Update the prompt file
- Add an example to the repository

Now that knowledge is **permanently encoded**. The next person—human or AI—learns it automatically by trying to violate it and being corrected by the system.

### 4. The codebase becomes a self-teaching machine

This is the ultimate goal: a codebase that actively teaches contributors how to work within it.

- Try to inline a type → CI suggests extracting it to the canonical location
- Try to hoist state too high → Linter explains the performance implications
- Try to add a redundant wrapper → Autofix removes it
- Try to use a non-standard pattern → AI agent shows the standard equivalent

The system doesn't just enforce correctness—it **explains correctness**. Each interaction makes the contributor slightly better at writing canonical code.

After a week of working in this environment, contributors internalize the patterns. They stop triggering the checks because they've learned to think the way the system thinks.

### 5. Humans and AI converge on identical output

This is where it gets interesting.

When the conventions are rigid enough and the automation is comprehensive enough, something remarkable happens: **humans and AI produce identical code**.

Not because humans are coding like machines—but because both are following the same canonical patterns, the same type signatures, the same folder structures, the same architectural principles.

A human writes a new component: it looks canonical.
An AI generates the same component: it looks identical.

This isn't loss of creativity—it's **creativity focused where it matters**. The structure is deterministic. The product logic is where humans add value.

### The vacation test as a design principle

This heuristic guides every architectural decision:

- Should this be configurable? *Only if someone on vacation couldn't reasonably guess the right value.*
- Should this be documented? *Only if the system can't enforce it automatically.*
- Should this be a convention? *Only if violating it would look different from code I'd write.*

The goal isn't perfection on day one. The goal is a **ratchet mechanism**: every time knowledge leaves my head and enters the system, it never has to leave my head again.

Over time, the percentage of PRs that "look exactly like code I would write" approaches 100%—whether I'm on vacation, asleep, or working on something else entirely.

**This is the ultimate form of leverage**: your architectural knowledge, multiplied across every contributor, human or AI, forever.

## Static Determinism Over AI Checks: The Performance Reality

There's a critical nuance to the "agentic CI" vision that many teams miss: **AI should build the checks, not be the checks.**

The technical reality is unavoidable: AI-based reviews and enforcement are inherently slow. Even at 0 temperature, token generation is orders of magnitude slower than code execution. An AI agent analyzing a PR takes seconds to minutes. A static linter checking the same patterns runs in milliseconds across hundreds of files.

**Static determinism always wins on latency.**

This isn't a temporary limitation—it's fundamental to how language models work. Every token must be generated sequentially. Every pattern must be reasoned about contextually. There's no way around the physics of inference.

But here's where the real leverage appears: **AI doesn't need to run the checks. AI needs to write the checks.**

### The paradigm shift

Traditional approach:
1. Human notices a pattern violation in code review
2. Human writes a comment explaining the issue
3. Human repeats this for every future occurrence
4. Knowledge transfer is manual and slow

The new approach:
1. Human notices a pattern violation
2. AI writes a custom static check (Biome rule, GritQL plugin, TypeScript rule)
3. Check runs in milliseconds on every future commit
4. Knowledge is permanently encoded

The difference is profound. Writing custom linter rules used to be prohibitively expensive:

- Understanding the AST parser API
- Writing complex pattern-matching logic
- Handling edge cases and false positives
- Testing across the codebase
- Maintaining the rule as the codebase evolves

This would take hours or days for a human. Most teams decided it wasn't worth it unless the bug was catastrophic.

**Now an AI can write a GritQL pattern or custom Biome rule in minutes.**

Suddenly the economics flip: if spending 10 minutes with AI to write a static check can prevent 1 hour of debugging over the next month, it's worth it. If that check catches 10 bugs over the next year, it's invaluable.

### The self-improving codebase flywheel

This creates a compounding effect:

1. **Issue appears in code review** → Developer and AI spend 15 minutes writing a static check
2. **Check catches the same issue** → Another developer fixes it pre-commit, learns the pattern
3. **Check runs on every commit** → Issue never appears again in any PR
4. **Time saved compounds** → Reviewers can focus on higher-level concerns

Each check added is a category of bugs permanently eliminated. Each pattern codified is knowledge that never needs to be re-explained.

Over time, the codebase becomes self-defending. The static analysis layer grows more sophisticated. The feedback loop gets faster.

### Real performance numbers

Consider the difference:

**AI-based PR review agent:**
- Fetches PR diff: 100-500ms
- Loads context: 500-2000ms
- Generates analysis: 5-30 seconds
- Per-file overhead: 2-5 seconds
- Total for 20-file PR: 60-120 seconds

**Static linter with custom rules:**
- Parse 100 files: 50-200ms
- Run all checks: 100-500ms
- Report violations: less than 10ms
- Total for entire codebase: less than 1 second

The static check is **60-120x faster**. And it runs locally before commit, not asynchronously in CI.

This isn't to say AI agents have no place in CI. They're perfect for:

- **Semantic review**: "Does this API design make sense?"
- **Architecture validation**: "Does this feature fit the existing patterns?"
- **Documentation checks**: "Are these type names consistent with our domain model?"

But for enforcing **known, repeatable patterns**, static checks win every time.

### Where AI truly excels: Abstract quality domains

Here's the crucial insight: **AI succeeds in highly abstract quality domains that resist formal specification.**

Static rules can enforce "no inline types" or "max 3 levels of nesting." But they struggle with:

- **Readability**: "This function is hard to follow"
- **Documentation quality**: "This explanation could be clearer"
- **UI aesthetics**: "This layout looks cramped"
- **Naming appropriateness**: "This variable name doesn't match the domain concept"
- **Code organization**: "This logic would be better elsewhere"

These are subjective, qualitative judgments that require taste and context. They're impossible to encode in deterministic rules, yet they're critical to code quality.

**This is where AI adds irreplaceable value.**

An AI reviewer can say: "This component is technically correct, but the logic flow is hard to follow. Consider extracting the data transformation into a named function to clarify intent."

No linter can make that judgment. No static analysis can detect "this is hard to read" in a generalizable way. But AI—trained on millions of examples of good and bad code—develops intuition for these abstract qualities.

The key is **separation of concerns**:

- **Static checks**: Enforce the mechanically verifiable (structure, types, patterns)
- **AI reviews**: Evaluate the qualitatively important (readability, clarity, aesthetics)

When you try to use AI for both, you get slow feedback loops and non-deterministic enforcement. When you use each tool for its strength, you get instant mechanical correctness *and* thoughtful qualitative guidance.

### AI in service of product decisions, guided by static determinism

The ideal architecture is:

**Fast feedback loop (static):**
- Biome for style and common patterns
- Custom GritQL for domain-specific rules
- TypeScript for type safety
- Custom ESLint plugins for React patterns
- Runs in milliseconds, blocks bad commits instantly

**Slow feedback loop (AI):**
- Architectural coherence
- API design review
- Documentation quality
- Complex pattern matching that's not worth codifying yet
- Runs asynchronously, provides guidance not gates

The static layer ensures the code is *structurally correct*. The AI layer ensures it's *semantically sound*.

Developers get instant feedback on mechanical correctness, allowing them to focus their cognitive energy on the nuanced product decisions where AI provides guidance but humans decide.

### The new value proposition

Before AI-assisted development:
- Custom static checks were too expensive to write
- Teams tolerated repeated violations
- Knowledge lived in reviewer heads
- Bugs were caught late in code review

After AI-assisted development:
- Custom checks can be written in minutes
- Teams codify patterns immediately
- Knowledge lives in executable rules
- Bugs are caught in milliseconds, pre-commit

**This is why canonical codebases are increasingly valuable**: each pattern you establish is a pattern AI can help you enforce statically, forever, at near-zero cost.

The codebase becomes a self-improving system where every bug caught once becomes a rule that prevents it permanently.

## The Hidden Cost of Infinite Architectural Freedom

React's unopinionated nature is celebrated as a strength, but it comes with a hidden tax: **decision fatigue at every level of the stack**.

When every team member can legitimately choose between five different approaches to the same problem, you don't get five times the innovation—you get five times the cognitive load. New engineers spend their first weeks not learning the domain, but decoding the archaeological layers of different patterns that emerged from different eras of the codebase.

This is not a problem that better documentation can solve. Documentation describes the current state; it doesn't prevent future divergence. Only rigid conventions can do that.

## Why Next.js Server Components Reinforce the Need for Consistency

The introduction of Server Components, Server Actions, and the App Router in Next.js has paradoxically made consistency *more* important, not less.

Now developers must decide not just *how* to fetch data, but *where*: Server Component, Client Component, Route Handler, Server Action, or middleware. Each choice has different implications for caching, revalidation, streaming, and user experience.

Without canonical patterns, teams devolve into inconsistent approaches:

- Some features fetch in Server Components
- Some use Client Components with `useEffect`
- Some rely on Route Handlers called from the client
- Some use Server Actions for mutations

Each approach works in isolation. But collectively, they create a codebase that's impossible to reason about holistically. Performance characteristics become unpredictable. Caching strategies conflict. Waterfall requests appear in surprising places.

**A canonical Next.js architecture establishes clear rules:**

- Server Components for initial data loading
- Client Components only when interactivity requires it
- Server Actions for mutations with built-in revalidation
- Route Handlers exclusively for external API consumers

These rules aren't restrictions—they're **decision elimination**. They collapse the search space from infinite to deterministic.

## How Conventions Encode Institutional Knowledge

When a senior engineer leaves, they take two types of knowledge with them:

1. **Explicit knowledge**: documented patterns, architectural decisions, design docs
2. **Implicit knowledge**: the "feel" for what's right, the instinct for where bugs hide, the judgment calls that never made it into docs

Traditional codebases lose the second type entirely. But canonical codebases *encode* implicit knowledge into the structure itself.

When the rule is "state lives at the lowest possible owner," you don't need to remember which engineer understood React's reconciliation deeply. The rule enforces the insight automatically.

When the rule is "no inline types," you don't need tribal knowledge about which shapes are stable versus which are evolving. The structure forces explicit naming, which forces explicit thinking.

**This is the ultimate scale advantage:** junior engineers inherit senior judgment without needing to rediscover it.

## The Psychology of Zero-Decision Environments and Flow State

Cognitive psychology research on flow state consistently shows that **reducing decisions increases sustained focus**.

Every architectural decision—no matter how small—pulls you out of implementation mode and into meta-reasoning mode. "Should this be a separate component? Should this type be inline or extracted? Should this file go in `components/` or `features/`?"

None of these decisions are inherently hard. But each one is a context switch. Each one burns glucose. Each one reduces your capacity for the decisions that actually matter: the product logic, the edge cases, the user experience.

A canonical codebase eliminates these micro-decisions. You don't choose where the file goes—the convention tells you. You don't choose how to structure the component—the pattern is already established. You don't choose how to name the type—the naming convention is deterministic.

**This is why velocity increases so dramatically.** Not because developers type faster, but because they spend more time in flow state and less time in decision paralysis.

## Why You Should Prefer More Files Over Longer Files

A 2000-line component file is not a victory. It's a liability.

Long files optimize for the wrong thing: minimizing the number of imports. But imports are essentially free—they're compile-time, they're explicit, and they're greppable.

What's expensive is mental state. When you open a 2000-line file, you must:

1. Scan to find the relevant section
2. Understand what scope you're in
3. Reason about side effects from distant code
4. Avoid accidentally depending on private implementation details

When you open a 200-line file, the entire file fits in your mental model at once.

**Canonical rule: Files should be small enough to hold in working memory.**

For most developers, that's 200-400 lines. Beyond that, split aggressively.

This has second-order benefits:

- **Better caching**: editing one small file doesn't invalidate unrelated code
- **Easier testing**: small files have smaller dependency graphs
- **Clearer boundaries**: file structure becomes semantic architecture
- **Faster AI reasoning**: models spend less time on irrelevant context

More files isn't complexity. It's **explicitness**. And explicitness is the foundation of correctness.

## How Decentralized Inline Types Sabotage Team-Level Understanding

Consider this pattern, common in ad-hoc codebases:

```typescript
// ComponentA.tsx
function ComponentA({ data }: { data: { id: string; name: string } }) {
  // ...
}

// ComponentB.tsx
function ComponentB({ item }: { item: { id: string; name: string } }) {
  // ...
}

// ComponentC.tsx
function ComponentC(props: { record: { id: string; name: string } }) {
  // ...
}
```

Three components, three inline types, three different parameter names—but the same semantic shape.

**What's the cost?**

1. **Searching is ambiguous.** Grep for `name: string` finds hundreds of matches. You can't locate all uses of this entity type.

2. **Refactoring is hazardous.** Adding a field requires touching every inline definition, and you'll inevitably miss one.

3. **Semantics are lost.** Is this a User? A Product? A Category? The shape doesn't say.

4. **AI cannot help.** The model sees hundreds of similar shapes with no canonical source of truth.

Now contrast with canonical types:

```typescript
// types/user.ts
export interface User {
  id: string;
  name: string;
}

// ComponentA.tsx
function ComponentA({ data }: { data: User }) {
  // ...
}

// ComponentB.tsx
function ComponentB({ item }: { item: User }) {
  // ...
}

// ComponentC.tsx
function ComponentC(props: { record: User }) {
  // ...
}
```

Now:

- Searching for `User` finds every usage
- Refactoring changes one file
- Semantics are explicit
- AI can reliably reason about the domain model

**The rule: If a type appears more than once, it must be named and exported.**

This isn't pedantry. It's an architectural guarantee that types reflect domain concepts, not implementation accidents.

## Why Minimal DOM Is the Mark of a Mature Engineering Team

Novice React developers wrap everything in `<div>`s. It feels safe—an extra wrapper never breaks anything.

But each wrapper has costs:

1. **Layout debugging becomes harder.** Flexbox and Grid work differently with intermediate containers.
2. **Accessibility suffers.** Screen readers traverse unnecessary nesting.
3. **Performance degrades.** More nodes mean longer reconciliation, slower paint, larger memory footprint.
4. **Code reviews slow down.** Reviewers must mentally prune irrelevant structure.

**The canonical rule: Every DOM element must justify its existence.**

- Does it style something? Then it's semantic.
- Does it position children? Then it's necessary.
- Does it exist "just in case"? **Delete it.**

Mature teams internalize this so deeply that their React components have nearly 1:1 correspondence with the semantic structure of the UI. There are no "wrapper divs." There are layout containers, content regions, and interactive elements—each with clear purpose.

This discipline pays dividends:

- **Diffs are smaller** because structural changes are rare
- **Styling is simpler** because selectors target meaningful elements
- **Performance is better** because reconciliation is cheaper
- **AI reasoning is faster** because there's no noise

The best React code looks almost like the design mockup's semantic structure, with no ceremonial layers.

## AI's Growing Voice as a Stakeholder in Code Quality

Traditionally, code quality optimizes for two audiences:

1. **The compiler** (correctness)
2. **Human maintainers** (readability)

But there's now a third audience: **AI assistants**.

This isn't hypothetical. If you use GitHub Copilot, Cursor, or Claude Code, your codebase is constantly being read by models that:

- Autocomplete based on surrounding context
- Suggest refactors based on patterns
- Generate tests based on implementation
- Answer questions based on structure

These tools work exponentially better on canonical codebases. Here's why:

**Predictable patterns compress well.** When every component follows the same structure, the model "learns" the pattern once and applies it everywhere. Inconsistent patterns require the model to re-learn from scratch for each file.

**Explicit types eliminate ambiguity.** Inline types force the model to infer intent. Named types provide ground truth.

**Small files fit in context windows.** A 200-line file with clear boundaries is fully comprehensible. A 2000-line file forces the model to sample fragments.

**Minimal DOM reduces noise.** Every redundant wrapper is tokens the model must process without gaining information.

**The insight: code quality is now a multi-player optimization.**

You're not just writing for future humans. You're writing for the AI that will help future humans understand your code.

Teams that ignore this are leaving velocity on the table. Teams that embrace it find their AI assistants become genuinely intelligent collaborators rather than glorified autocomplete.

## Practical Lint Rules That Enforce Correctness Through Structure

Canonical codebases don't rely on human discipline—they rely on automation. Here are lint rules that enforce structural correctness:

### 1. **No default exports**

```typescript
// ❌ Bad
export default function Component() { }

// ✅ Good
export function Component() { }
```

**Why:** Default exports hide naming at the import site, making refactors fragile and search unreliable.

### 2. **No deep nesting (max 3 levels)**

```typescript
// ❌ Bad
<div>
  <div>
    <div>
      <div>
        <Component />
      </div>
    </div>
  </div>
</div>

// ✅ Good
<div>
  <Component />
</div>
```

**Why:** Deep nesting indicates missing abstractions.

### 3. **Props must be interfaces, not inline types**

```typescript
// ❌ Bad
function Component({ name }: { name: string }) { }

// ✅ Good
interface ComponentProps {
  name: string;
}
function Component({ name }: ComponentProps) { }
```

**Why:** Named interfaces enable reuse and explicit contracts.

### 4. **No useState for derived values**

```typescript
// ❌ Bad
const [fullName, setFullName] = useState(`${first} ${last}`);

// ✅ Good
const fullName = `${first} ${last}`;
```

**Why:** Derived state causes synchronization bugs.

### 5. **Component files must export exactly one component**

**Why:** Single Responsibility Principle at the file level. Easier to locate, test, and refactor.

These rules aren't oppressive—they're **forcing functions** that prevent entire categories of bugs before they're written.

## How Consistent Patterns Unlock Reliable Automated Refactors

The promise of automated refactoring tools has always been limited by ambiguity. How do you safely rename a prop when it's defined inline in 50 places?

Canonical codebases solve this by **eliminating structural ambiguity**.

When every component follows the same pattern:

- TypeScript's language server can reliably rename types
- ESLint's autofixers can safely restructure code
- Codemods can mechanically transform entire features

**Example: Adding a required prop**

In an ad-hoc codebase, this requires manually finding every usage and updating inline types. In a canonical codebase:

1. Add the prop to the shared interface
2. TypeScript errors at every call site
3. Autofix generates correct defaults
4. Tests catch edge cases

The refactor is **mechanical, not intellectual**. This is the goal: reduce the human to a sanity-checker, not a manual laborer.

As AI-powered refactoring tools mature, this advantage compounds. Models can safely apply transformations to canonical codebases that would be hazardous in inconsistent ones.

**The insight: consistency enables automation, and automation enables velocity.**

## The Closing Argument: Opinionated Codebases Are the Future

Teams that treat conventions as optional will find themselves constantly firefighting.

Teams that aggressively canonicalize React/Next.js will find themselves building faster, debugging less, and collaborating seamlessly with AI.

**Correctness, performance, readability, and AI-friendliness all converge on the same set of architectural principles.**

A canonical React/Next.js style guide isn't rigid—it's liberating. It transforms the codebase from a collection of files into a coherent system.

When every file looks familiar, every pattern is predictable, and every rule eliminates a category of bugs, what you're left with is not restriction—it's **creative freedom within a powerful framework**.

The cognitive load you save on structure is cognitive load you can spend on product, on UX, on the problems that actually matter.

This is the future: codebases that are not just correct, but *canonically correct*. Where the right way is the obvious way. Where humans and AI collaborate fluently. Where velocity comes not from cutting corners, but from eliminating the corners entirely.

Build canonical systems. The returns compound forever.
